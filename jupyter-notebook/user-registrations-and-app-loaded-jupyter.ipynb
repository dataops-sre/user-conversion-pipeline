{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User registrations and app-loaded exploration Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL wikipedia pageview\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glance into the data and get the data schema out of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "|browser_version|campaign|channel|device_type|event     |initiator_id       |timestamp               |\n",
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "|null           |null    |null   |null       |registered|3074457347135400447|2020-01-08T06:21:14.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457345816644047|2020-01-08T06:24:42.000Z|\n",
      "|               |null    |null   |tablet-app |app_loaded|3074457346184244610|2020-01-08T06:25:10.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347135385819|2020-01-08T06:25:11.000Z|\n",
      "|78.0           |null    |null   |desktop    |app_loaded|3074457346246864126|2020-01-08T06:27:23.000Z|\n",
      "|76.0           |null    |null   |desktop    |app_loaded|3074457346612629694|2020-01-08T17:54:39.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347100151330|2020-01-08T17:55:07.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347067835477|2020-01-08T17:55:54.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457345711630326|2020-01-08T17:57:06.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457346358050812|2020-01-08T17:57:19.000Z|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pickup data for dev\n",
    "data_input = \"./data-input\"\n",
    "df = spark.read.json(data_input)\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('browser_version', StringType(), True), StructField('campaign', StringType(), True), StructField('channel', StringType(), True), StructField('device_type', StringType(), True), StructField('event', StringType(), True), StructField('initiator_id', LongType(), True), StructField('timestamp', StringType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define schema according to the exploration\n",
    "`|browser_version|campaign|channel|device_type|event|initiator_id|timestamp|`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "StructType([StructField('browser_version', StringType(), True), StructField('campaign', StringType(), True), StructField('channel', StringType(), True), StructField('device_type', StringType(), True), StructField('event', StringType(), True), StructField('initiator_id', LongType(), True), StructField('timestamp', StringType(), True)])\n",
    "'''\n",
    "from pyspark.sql.types import (\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "input_data_schema = StructType() \\\n",
    "      .add(\"browser_version\",StringType(),True) \\\n",
    "      .add(\"campaign\",StringType(),True) \\\n",
    "      .add(\"channel\",StringType(),True) \\\n",
    "      .add(\"device_type\",StringType(),True) \\\n",
    "      .add(\"event\",StringType(),True) \\\n",
    "      .add(\"initiator_id\",LongType(),True) \\\n",
    "      .add(\"timestamp\",TimestampType(),True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data again with data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "|browser_version|campaign|channel|device_type|     event|       initiator_id|          timestamp|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "|           null|    null|   null|       null|registered|3074457347135400447|2020-01-08 06:21:14|\n",
      "|           79.0|    null|   null|    desktop|app_loaded|3074457345816644047|2020-01-08 06:24:42|\n",
      "|               |    null|   null| tablet-app|app_loaded|3074457346184244610|2020-01-08 06:25:10|\n",
      "|           79.0|    null|   null|    desktop|app_loaded|3074457347135385819|2020-01-08 06:25:11|\n",
      "|           78.0|    null|   null|    desktop|app_loaded|3074457346246864126|2020-01-08 06:27:23|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", True).schema(input_data_schema) \\\n",
    "    .json(data_input)\n",
    "    \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataframe into two sub dataframes -- user_registration and app_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_registration_df = df.select(\"event\",\n",
    "                                 \"timestamp\",\n",
    "                                 \"initiator_id\",\n",
    "                                 \"channel\"\n",
    "                                )\n",
    "app_loaded_df = df.select(\"event\",\n",
    "                          \"timestamp\",\n",
    "                          \"initiator_id\",\n",
    "                          \"device_type\"\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation then Write dataframes partition by day in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    date_format\n",
    ")\n",
    "\n",
    "user_registration_df = user_registration_df.withColumn(\n",
    "        \"derived_tstamp_day\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
    "    ).withColumnRenamed(\n",
    "    'timestamp', 'time'\n",
    "    )\n",
    "\n",
    "app_loaded_df = app_loaded_df.withColumn(\n",
    "        \"derived_tstamp_day\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
    "    ).withColumnRenamed(\n",
    "    'timestamp', 'time'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"data-output/user_registration\"\n",
    "\n",
    "user_registration_df.repartition(1)\\\n",
    "  .write.option(\"compression\", \"snappy\")\\\n",
    "  .save(\n",
    "    path=write_path,\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=\"derived_tstamp_day\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"data-output/app_loaded\"\n",
    "\n",
    "app_loaded_df.repartition(1)\\\n",
    "  .write.option(\"compression\", \"snappy\")\\\n",
    "  .save(\n",
    "    path=write_path,\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=\"derived_tstamp_day\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
