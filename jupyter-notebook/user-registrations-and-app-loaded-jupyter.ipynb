{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User registrations and app-loaded exploration Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL scatch pad\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task one: events split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glance into the data and get the data schema out of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "|browser_version|campaign|channel|device_type|event     |initiator_id       |timestamp               |\n",
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "|null           |null    |null   |null       |registered|3074457347135400447|2020-01-08T06:21:14.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457345816644047|2020-01-08T06:24:42.000Z|\n",
      "|               |null    |null   |tablet-app |app_loaded|3074457346184244610|2020-01-08T06:25:10.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347135385819|2020-01-08T06:25:11.000Z|\n",
      "|78.0           |null    |null   |desktop    |app_loaded|3074457346246864126|2020-01-08T06:27:23.000Z|\n",
      "|76.0           |null    |null   |desktop    |app_loaded|3074457346612629694|2020-01-08T17:54:39.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347100151330|2020-01-08T17:55:07.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347067835477|2020-01-08T17:55:54.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457345711630326|2020-01-08T17:57:06.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457346358050812|2020-01-08T17:57:19.000Z|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pickup input data data \n",
    "data_input = \"./data-input\"\n",
    "df = spark.read.json(data_input)\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('browser_version', StringType(), True), StructField('campaign', StringType(), True), StructField('channel', StringType(), True), StructField('device_type', StringType(), True), StructField('event', StringType(), True), StructField('initiator_id', LongType(), True), StructField('timestamp', StringType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define schema according to the exploration\n",
    "`|browser_version|campaign|channel|device_type|event|initiator_id|timestamp|`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "StructType([StructField('browser_version', StringType(), True), StructField('campaign', StringType(), True), StructField('channel', StringType(), True), StructField('device_type', StringType(), True), StructField('event', StringType(), True), StructField('initiator_id', LongType(), True), StructField('timestamp', StringType(), True)])\n",
    "'''\n",
    "from pyspark.sql.types import (\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "input_data_schema = StructType() \\\n",
    "      .add(\"browser_version\",StringType(),True) \\\n",
    "      .add(\"campaign\",StringType(),True) \\\n",
    "      .add(\"channel\",StringType(),True) \\\n",
    "      .add(\"device_type\",StringType(),True) \\\n",
    "      .add(\"event\",StringType(),True) \\\n",
    "      .add(\"initiator_id\",LongType(),True) \\\n",
    "      .add(\"timestamp\",TimestampType(),True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data again with data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "|browser_version|campaign|channel|device_type|     event|       initiator_id|          timestamp|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "|           null|    null|   null|       null|registered|3074457347135400447|2020-01-08 06:21:14|\n",
      "|           79.0|    null|   null|    desktop|app_loaded|3074457345816644047|2020-01-08 06:24:42|\n",
      "|               |    null|   null| tablet-app|app_loaded|3074457346184244610|2020-01-08 06:25:10|\n",
      "|           79.0|    null|   null|    desktop|app_loaded|3074457347135385819|2020-01-08 06:25:11|\n",
      "|           78.0|    null|   null|    desktop|app_loaded|3074457346246864126|2020-01-08 06:27:23|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", True).schema(input_data_schema) \\\n",
    "    .json(data_input)\n",
    "    \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataframe into two sub dataframes -- user_registration and app_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_registration_df = df.select(\"event\",\n",
    "                                 \"timestamp\",\n",
    "                                 \"initiator_id\",\n",
    "                                 \"channel\"\n",
    "                                ).where(df.event == \"registered\")\n",
    "app_loaded_df = df.select(\"event\",\n",
    "                          \"timestamp\",\n",
    "                          \"initiator_id\",\n",
    "                          \"device_type\"\n",
    "                          ).where(df.event == \"app_loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation then Write dataframes partition by day in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    date_format\n",
    ")\n",
    "\n",
    "user_registration_df = user_registration_df.withColumn(\n",
    "        \"derived_tstamp_day\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
    "    ).withColumnRenamed(\n",
    "    'timestamp', 'time'\n",
    "    )\n",
    "\n",
    "app_loaded_df = app_loaded_df.withColumn(\n",
    "        \"derived_tstamp_day\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
    "    ).withColumnRenamed(\n",
    "    'timestamp', 'time'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|     event|               time|       initiator_id|channel|derived_tstamp_day|\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|registered|2020-01-08 06:21:14|3074457347135400447|   null|        2020-01-08|\n",
      "|registered|2020-01-08 17:55:42|3074457347136974015|   null|        2020-01-08|\n",
      "|registered|2020-01-08 23:49:14|3074457347138054179|   null|        2020-01-08|\n",
      "|registered|2020-01-08 12:48:21|3074457347125446331| invite|        2020-01-08|\n",
      "|registered|2020-01-08 12:52:17|3074457347125292449| invite|        2020-01-08|\n",
      "|registered|2020-01-08 02:41:00|3074457347135206851|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:32:39|3074457347135939667|   null|        2020-01-08|\n",
      "|registered|2020-01-08 13:50:06|3074457347136484019| invite|        2020-01-08|\n",
      "|registered|2020-01-08 12:08:15|3074457347136224280|   null|        2020-01-08|\n",
      "|registered|2020-01-08 12:13:44|3074457347136224255|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:10:59|3074457347135907944|   null|        2020-01-08|\n",
      "|registered|2020-01-08 15:31:34|3074457347136709819|   null|        2020-01-08|\n",
      "|registered|2020-01-08 20:09:31|3074457347135034862|   null|        2020-01-08|\n",
      "|registered|2020-01-08 20:15:51|3074457347110576426| invite|        2020-01-08|\n",
      "|registered|2020-01-08 14:38:27|3074457347136709545|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:38:51|3074457347135985213|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:40:43|3074457347135985430|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:09:25|3074457347135627402| invite|        2020-01-08|\n",
      "|registered|2020-01-08 10:10:10|3074457347135895149|   null|        2020-01-08|\n",
      "|registered|2020-01-08 15:57:27|3074457347137019248|   null|        2020-01-08|\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"event\":\"registered\",\"time\":\"2020-01-08T06:21:14.000Z\",\"initiator_id\":3074457347135400447,\"derived_tstamp_day\":\"2020-01-08\"}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_registration_df.show()\n",
    "user_registration_df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"data-output/user_registration\"\n",
    "\n",
    "user_registration_df.repartition(1)\\\n",
    "  .write.option(\"compression\", \"snappy\")\\\n",
    "  .save(\n",
    "    path=write_path,\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=\"derived_tstamp_day\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"data-output/app_loaded\"\n",
    "\n",
    "app_loaded_df.repartition(1)\\\n",
    "  .write.option(\"compression\", \"snappy\")\\\n",
    "  .save(\n",
    "    path=write_path,\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=\"derived_tstamp_day\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task two: customer journey -- next-week-conversion-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|event     |time               |initiator_id       |channel|derived_tstamp_day|\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|registered|2020-01-28 14:19:36|3074457347192467026|null   |2020-01-28        |\n",
      "|registered|2020-01-28 16:38:31|3074457347193109377|null   |2020-01-28        |\n",
      "|registered|2020-01-28 19:03:38|3074457347192713430|invite |2020-01-28        |\n",
      "|registered|2020-01-28 15:01:18|3074457347192562179|invite |2020-01-28        |\n",
      "|registered|2020-01-28 15:03:32|3074457347191986668|invite |2020-01-28        |\n",
      "|registered|2020-01-28 08:19:09|3074457347191225250|null   |2020-01-28        |\n",
      "|registered|2020-01-28 17:22:48|3074457347177297037|invite |2020-01-28        |\n",
      "|registered|2020-01-28 14:04:30|3074457347191921811|invite |2020-01-28        |\n",
      "|registered|2020-01-28 07:46:36|3074457347187303104|invite |2020-01-28        |\n",
      "|registered|2020-01-28 12:41:03|3074457347191883897|null   |2020-01-28        |\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pickup input data data \n",
    "user_registration_data_input = \"./data-output/user_registration\"\n",
    "udf = spark.read.parquet(user_registration_data_input)\n",
    "udf.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there's any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7318"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7317"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf.select(\"initiator_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what the duplicate looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|       initiator_id|count|\n",
      "+-------------------+-----+\n",
      "|3074457347132582884|    2|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    count,\n",
    ")\n",
    "x = udf.groupby(\"initiator_id\").agg(count('initiator_id').alias('count')).filter(col('count')>1)\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|     event|               time|       initiator_id|channel|derived_tstamp_day|\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|registered|2020-01-07 02:20:49|3074457347132582884|   null|        2020-01-07|\n",
      "|registered|2020-01-07 02:20:50|3074457347132582884|   null|        2020-01-07|\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf.filter(col(\"initiator_id\")==3074457347132582884).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicates might come from frontend tracker? Take the earlier event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+----------+------------------+\n",
      "|       initiator_id|               time|channel|     event|derived_tstamp_day|\n",
      "+-------------------+-------------------+-------+----------+------------------+\n",
      "|3074457345618261067|2020-01-10 12:39:25| direct|registered|        2020-01-10|\n",
      "|3074457345618261107|2020-01-10 12:40:45| direct|registered|        2020-01-10|\n",
      "|3074457345618261140|2020-01-10 12:55:00| direct|registered|        2020-01-10|\n",
      "|3074457345618261281|2020-01-10 13:09:14| direct|registered|        2020-01-10|\n",
      "|3074457345618261321|2020-01-10 13:10:33| direct|registered|        2020-01-10|\n",
      "+-------------------+-------------------+-------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "x = udf.groupby(\"initiator_id\").agg(F.min('time').alias(\"time\"),\n",
    "                                    F.first(\"channel\").alias(\"channel\"),\n",
    "                                    F.first(\"event\").alias(\"event\"),\n",
    "                                    F.first(\"derived_tstamp_day\").alias(\"derived_tstamp_day\")\n",
    "                                   )\n",
    "x.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----------+------------------+\n",
      "|event     |time               |initiator_id       |device_type|derived_tstamp_day|\n",
      "+----------+-------------------+-------------------+-----------+------------------+\n",
      "|app_loaded|2020-01-29 06:30:14|3074457346185514918|desktop    |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:31:49|3074457345894038970|desktop-app|2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:32:16|3074457347170125143|mobile-app |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:33:42|3074457347115508571|mobile-app |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:34:54|3074457346921032451|desktop    |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:35:21|3074457347085959347|desktop    |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:35:27|3074457346118464419|desktop    |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:36:19|3074457346554502961|desktop    |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:36:40|3074457347104913825|mobile-app |2020-01-29        |\n",
      "|app_loaded|2020-01-29 06:36:51|3074457346612218640|mobile-app |2020-01-29        |\n",
      "+----------+-------------------+-------------------+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pickup app_loaded input data data \n",
    "app_loaded_data_input = \"./data-output/app_loaded\"\n",
    "adf = spark.read.parquet(app_loaded_data_input)\n",
    "adf.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the earlier app_loaded event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+----------+------------------+\n",
      "|initiator_id|               time|device_type|     event|derived_tstamp_day|\n",
      "+------------+-------------------+-----------+----------+------------------+\n",
      "|       70525|2020-01-09 05:21:09|    desktop|app_loaded|        2020-01-29|\n",
      "|       70529|2020-01-27 06:10:35|    desktop|app_loaded|        2020-01-28|\n",
      "|       70593|2020-01-20 08:52:44|    desktop|app_loaded|        2020-01-30|\n",
      "|       85072|2020-01-01 17:03:40|    desktop|app_loaded|        2020-01-01|\n",
      "|       89245|2020-01-08 13:27:46|    desktop|app_loaded|        2020-01-29|\n",
      "+------------+-------------------+-----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = adf.groupby(\"initiator_id\").agg(F.min('time').alias(\"time\"),\n",
    "                                    F.first(\"device_type\").alias(\"device_type\"),\n",
    "                                    F.first(\"event\").alias(\"event\"),\n",
    "                                    F.first(\"derived_tstamp_day\").alias(\"derived_tstamp_day\"))\n",
    "y.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the user conversion dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------------+-------------------+\n",
      "|       initiator_id|channel|                 ut|                 at|\n",
      "+-------------------+-------+-------------------+-------------------+\n",
      "|3074457345618261067| direct|2020-01-10 12:39:25|               null|\n",
      "|3074457345618261107| direct|2020-01-10 12:40:45|               null|\n",
      "|3074457345618261140| direct|2020-01-10 12:55:00|               null|\n",
      "|3074457345618261281| direct|2020-01-10 13:09:14|               null|\n",
      "|3074457345618261321| direct|2020-01-10 13:10:33|               null|\n",
      "|3074457345618261401| direct|2020-01-10 13:15:31|               null|\n",
      "|3074457345618261562| direct|2020-01-10 13:41:45|               null|\n",
      "|3074457345618263616| direct|2020-01-10 13:53:37|               null|\n",
      "|3074457345618263690| direct|2020-01-10 13:54:26|               null|\n",
      "|3074457345618264127| direct|2020-01-10 13:55:10|               null|\n",
      "|3074457345618264150| direct|2020-01-10 13:55:49|               null|\n",
      "|3074457345618265450| direct|2020-01-10 16:19:25|               null|\n",
      "|3074457345618275274| direct|2020-01-14 06:43:35|               null|\n",
      "|3074457345618275302| direct|2020-01-14 07:11:32|               null|\n",
      "|3074457345618285606| direct|2020-01-14 19:53:13|               null|\n",
      "|3074457345618285622| direct|2020-01-14 20:11:45|               null|\n",
      "|3074457345618292021| direct|2020-01-17 05:22:01|               null|\n",
      "|3074457345618420279| direct|2020-01-20 08:05:36|               null|\n",
      "|3074457346958960761| invite|2020-01-16 02:10:58|               null|\n",
      "|3074457346976372564| invite|2020-01-06 12:51:37|2020-01-06 12:58:44|\n",
      "+-------------------+-------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resd=x.join(y, x.initiator_id == y.initiator_id, 'left').select(x.initiator_id, x.channel, x.time.alias(\"ut\"), y.time.alias(\"at\"))\n",
    "resd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is user conversion rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28030613639469726\n"
     ]
    }
   ],
   "source": [
    "print(resd.filter(F.col(\"at\").isNotNull()).count() / resd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion rate is way too low... is it related to register channel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|channel|\n",
      "+-------+\n",
      "|   null|\n",
      "| direct|\n",
      "| invite|\n",
      "+-------+\n",
      "\n",
      "% of invited registration: 0.2950662839961733\n",
      "% of direct registration: 0.0024600246002460025\n",
      "% of null channel registration: 0.7024736914035807\n"
     ]
    }
   ],
   "source": [
    "resd.select(\"channel\").distinct().show()\n",
    "\n",
    "print(\"% of invited registration:\", resd.filter(F.col(\"channel\")==\"invite\").count() / resd.count())\n",
    "print(\"% of direct registration:\", resd.filter(F.col(\"channel\")==\"direct\").count() / resd.count())\n",
    "print(\"% of null channel registration:\", resd.filter(F.col(\"channel\").isNull()).count() / resd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check conversion rate of each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversion rate of the invited registration: 0.3524779990736452\n",
      "conversion rate of the direct registration: 0.0\n",
      "conversion rate of the null channel registration: 0.2509727626459144\n"
     ]
    }
   ],
   "source": [
    "print(\"conversion rate of the invited registration:\", resd.filter((F.col(\"channel\")==\"invite\") & (F.col(\"at\").isNotNull())).count() / resd.filter(F.col(\"channel\")==\"invite\").count())\n",
    "print(\"conversion rate of the direct registration:\", resd.filter((F.col(\"channel\")==\"direct\") & (F.col(\"at\").isNotNull())).count() / resd.filter(F.col(\"channel\")==\"direct\").count())\n",
    "print(\"conversion rate of the null channel registration:\", resd.filter((F.col(\"channel\").isNull()) & (F.col(\"at\").isNotNull())).count() / resd.filter(F.col(\"channel\").isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like conversion rate does not related to channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7318"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325478"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7317"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trunc the week to prepare for week_diff column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------------+-------------------+---------+\n",
      "|       initiator_id|channel|                 ut|                 at|week_diff|\n",
      "+-------------------+-------+-------------------+-------------------+---------+\n",
      "|3074457345618261067| direct|2020-01-10 12:39:25|               null|     null|\n",
      "|3074457345618261107| direct|2020-01-10 12:40:45|               null|     null|\n",
      "|3074457345618261140| direct|2020-01-10 12:55:00|               null|     null|\n",
      "|3074457345618261281| direct|2020-01-10 13:09:14|               null|     null|\n",
      "|3074457345618261321| direct|2020-01-10 13:10:33|               null|     null|\n",
      "|3074457345618261401| direct|2020-01-10 13:15:31|               null|     null|\n",
      "|3074457345618261562| direct|2020-01-10 13:41:45|               null|     null|\n",
      "|3074457345618263616| direct|2020-01-10 13:53:37|               null|     null|\n",
      "|3074457345618263690| direct|2020-01-10 13:54:26|               null|     null|\n",
      "|3074457345618264127| direct|2020-01-10 13:55:10|               null|     null|\n",
      "|3074457345618264150| direct|2020-01-10 13:55:49|               null|     null|\n",
      "|3074457345618265450| direct|2020-01-10 16:19:25|               null|     null|\n",
      "|3074457345618275274| direct|2020-01-14 06:43:35|               null|     null|\n",
      "|3074457345618275302| direct|2020-01-14 07:11:32|               null|     null|\n",
      "|3074457345618285606| direct|2020-01-14 19:53:13|               null|     null|\n",
      "|3074457345618285622| direct|2020-01-14 20:11:45|               null|     null|\n",
      "|3074457345618292021| direct|2020-01-17 05:22:01|               null|     null|\n",
      "|3074457345618420279| direct|2020-01-20 08:05:36|               null|     null|\n",
      "|3074457346958960761| invite|2020-01-16 02:10:58|               null|     null|\n",
      "|3074457346976372564| invite|2020-01-06 12:51:37|2020-01-06 12:58:44|        0|\n",
      "+-------------------+-------+-------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = resd.withColumn('week_diff',(F.datediff(F.trunc(\"at\",\"week\"), F.trunc(\"ut\",\"week\"))/7).cast('int'))\n",
    "a.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05125051250512505\n",
      "0.2047287139538062\n"
     ]
    }
   ],
   "source": [
    "print(a.where(F.col(\"week_diff\") == 1).count()/a.count())\n",
    "print(a.where(F.col(\"week_diff\") == 0).count()/a.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------------+----+---------+\n",
      "|       initiator_id|channel|                 ut|  at|week_diff|\n",
      "+-------------------+-------+-------------------+----+---------+\n",
      "|3074457345618261067| direct|2020-01-10 12:39:25|null|     null|\n",
      "|3074457345618261107| direct|2020-01-10 12:40:45|null|     null|\n",
      "|3074457345618261140| direct|2020-01-10 12:55:00|null|     null|\n",
      "|3074457345618261281| direct|2020-01-10 13:09:14|null|     null|\n",
      "|3074457345618261321| direct|2020-01-10 13:10:33|null|     null|\n",
      "+-------------------+-------+-------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+--------+-------+-----------+----------+-------------------+--------------------+\n",
      "|browser_version|campaign|channel|device_type|     event|       initiator_id|           timestamp|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+--------------------+\n",
      "|           null|        | direct|       null|registered|3074457345618285622|2020-01-14T20:11:...|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show(5)\n",
    "df.where(F.col(\"initiator_id\") == 3074457345618285622).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
