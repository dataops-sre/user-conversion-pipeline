{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User registrations and app-loaded exploration Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL wikipedia pageview\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glance into the data and get the data schema out of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "|browser_version|campaign|channel|device_type|event     |initiator_id       |timestamp               |\n",
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "|null           |null    |null   |null       |registered|3074457347135400447|2020-01-08T06:21:14.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457345816644047|2020-01-08T06:24:42.000Z|\n",
      "|               |null    |null   |tablet-app |app_loaded|3074457346184244610|2020-01-08T06:25:10.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347135385819|2020-01-08T06:25:11.000Z|\n",
      "|78.0           |null    |null   |desktop    |app_loaded|3074457346246864126|2020-01-08T06:27:23.000Z|\n",
      "|76.0           |null    |null   |desktop    |app_loaded|3074457346612629694|2020-01-08T17:54:39.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347100151330|2020-01-08T17:55:07.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457347067835477|2020-01-08T17:55:54.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457345711630326|2020-01-08T17:57:06.000Z|\n",
      "|79.0           |null    |null   |desktop    |app_loaded|3074457346358050812|2020-01-08T17:57:19.000Z|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pickup data for dev\n",
    "data_input = \"./data-input\"\n",
    "df = spark.read.json(data_input)\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('browser_version', StringType(), True), StructField('campaign', StringType(), True), StructField('channel', StringType(), True), StructField('device_type', StringType(), True), StructField('event', StringType(), True), StructField('initiator_id', LongType(), True), StructField('timestamp', StringType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define schema according to the exploration\n",
    "`|browser_version|campaign|channel|device_type|event|initiator_id|timestamp|`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "StructType([StructField('browser_version', StringType(), True), StructField('campaign', StringType(), True), StructField('channel', StringType(), True), StructField('device_type', StringType(), True), StructField('event', StringType(), True), StructField('initiator_id', LongType(), True), StructField('timestamp', StringType(), True)])\n",
    "'''\n",
    "from pyspark.sql.types import (\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "input_data_schema = StructType() \\\n",
    "      .add(\"browser_version\",StringType(),True) \\\n",
    "      .add(\"campaign\",StringType(),True) \\\n",
    "      .add(\"channel\",StringType(),True) \\\n",
    "      .add(\"device_type\",StringType(),True) \\\n",
    "      .add(\"event\",StringType(),True) \\\n",
    "      .add(\"initiator_id\",LongType(),True) \\\n",
    "      .add(\"timestamp\",TimestampType(),True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data again with data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "|browser_version|campaign|channel|device_type|     event|       initiator_id|          timestamp|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "|           null|    null|   null|       null|registered|3074457347135400447|2020-01-08 06:21:14|\n",
      "|           79.0|    null|   null|    desktop|app_loaded|3074457345816644047|2020-01-08 06:24:42|\n",
      "|               |    null|   null| tablet-app|app_loaded|3074457346184244610|2020-01-08 06:25:10|\n",
      "|           79.0|    null|   null|    desktop|app_loaded|3074457347135385819|2020-01-08 06:25:11|\n",
      "|           78.0|    null|   null|    desktop|app_loaded|3074457346246864126|2020-01-08 06:27:23|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", True).schema(input_data_schema) \\\n",
    "    .json(data_input)\n",
    "    \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataframe into two sub dataframes -- user_registration and app_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_registration_df = df.select(\"event\",\n",
    "                                 \"timestamp\",\n",
    "                                 \"initiator_id\",\n",
    "                                 \"channel\"\n",
    "                                ).where(df.event == \"registered\")\n",
    "app_loaded_df = df.select(\"event\",\n",
    "                          \"timestamp\",\n",
    "                          \"initiator_id\",\n",
    "                          \"device_type\"\n",
    "                          ).where(df.event == \"app_loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation then Write dataframes partition by day in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    date_format\n",
    ")\n",
    "\n",
    "user_registration_df = user_registration_df.withColumn(\n",
    "        \"derived_tstamp_day\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
    "    ).withColumnRenamed(\n",
    "    'timestamp', 'time'\n",
    "    )\n",
    "\n",
    "app_loaded_df = app_loaded_df.withColumn(\n",
    "        \"derived_tstamp_day\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
    "    ).withColumnRenamed(\n",
    "    'timestamp', 'time'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|     event|               time|       initiator_id|channel|derived_tstamp_day|\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "|registered|2020-01-08 06:21:14|3074457347135400447|   null|        2020-01-08|\n",
      "|registered|2020-01-08 17:55:42|3074457347136974015|   null|        2020-01-08|\n",
      "|registered|2020-01-08 23:49:14|3074457347138054179|   null|        2020-01-08|\n",
      "|registered|2020-01-08 12:48:21|3074457347125446331| invite|        2020-01-08|\n",
      "|registered|2020-01-08 12:52:17|3074457347125292449| invite|        2020-01-08|\n",
      "|registered|2020-01-08 02:41:00|3074457347135206851|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:32:39|3074457347135939667|   null|        2020-01-08|\n",
      "|registered|2020-01-08 13:50:06|3074457347136484019| invite|        2020-01-08|\n",
      "|registered|2020-01-08 12:08:15|3074457347136224280|   null|        2020-01-08|\n",
      "|registered|2020-01-08 12:13:44|3074457347136224255|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:10:59|3074457347135907944|   null|        2020-01-08|\n",
      "|registered|2020-01-08 15:31:34|3074457347136709819|   null|        2020-01-08|\n",
      "|registered|2020-01-08 20:09:31|3074457347135034862|   null|        2020-01-08|\n",
      "|registered|2020-01-08 20:15:51|3074457347110576426| invite|        2020-01-08|\n",
      "|registered|2020-01-08 14:38:27|3074457347136709545|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:38:51|3074457347135985213|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:40:43|3074457347135985430|   null|        2020-01-08|\n",
      "|registered|2020-01-08 10:09:25|3074457347135627402| invite|        2020-01-08|\n",
      "|registered|2020-01-08 10:10:10|3074457347135895149|   null|        2020-01-08|\n",
      "|registered|2020-01-08 15:57:27|3074457347137019248|   null|        2020-01-08|\n",
      "+----------+-------------------+-------------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"event\":\"registered\",\"time\":\"2020-01-08T06:21:14.000Z\",\"initiator_id\":3074457347135400447,\"derived_tstamp_day\":\"2020-01-08\"}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_registration_df.show()\n",
    "user_registration_df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"data-output/user_registration\"\n",
    "\n",
    "user_registration_df.repartition(1)\\\n",
    "  .write.option(\"compression\", \"snappy\")\\\n",
    "  .save(\n",
    "    path=write_path,\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=\"derived_tstamp_day\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"data-output/app_loaded\"\n",
    "\n",
    "app_loaded_df.repartition(1)\\\n",
    "  .write.option(\"compression\", \"snappy\")\\\n",
    "  .save(\n",
    "    path=write_path,\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=\"derived_tstamp_day\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytest\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytest'"
     ]
    }
   ],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from pytest) (22.1.0)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.0.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest) (21.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest) (2.0.1)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pytest) (3.0.9)\n",
      "Installing collected packages: iniconfig, pluggy, exceptiongroup, pytest\n",
      "Successfully installed exceptiongroup-1.0.0 iniconfig-1.1.1 pluggy-1.0.0 pytest-7.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-spark\n",
      "  Downloading pytest_spark-0.6.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (from pytest-spark) (7.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest->pytest-spark) (21.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest->pytest-spark) (1.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from pytest->pytest-spark) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest->pytest-spark) (2.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from pytest->pytest-spark) (22.1.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest->pytest-spark) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pytest->pytest-spark) (3.0.9)\n",
      "Installing collected packages: findspark, pytest-spark\n",
      "Successfully installed findspark-2.0.1 pytest-spark-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
